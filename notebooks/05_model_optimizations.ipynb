{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bog-9ygSgek6"
      },
      "source": [
        "# Optimizing the model with series of experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9wLOENogek9"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEeiI5Ekgek-"
      },
      "source": [
        "### LetÂ´s first reload the data from the previous notebook. Then, perform train_test_split once again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHwaGfg_gek_"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = pathlib.Path.cwd().parent / 'data'\n",
        "print(DATA_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGH-oL7rgek_"
      },
      "outputs": [],
      "source": [
        "model_data_scaled_path = DATA_DIR / 'processed' / 'ames_model_data_scaled.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7Y5Iacjgek_"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle(model_data_scaled_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj5g9uDugek_"
      },
      "outputs": [],
      "source": [
        "X = data.drop(columns=['SalePrice']).copy().values\n",
        "y = data['SalePrice'].copy().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siGQ3G7ggelA"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nm7_WOdgelA"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42  # Any number here, really.\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.25,\n",
        "    random_state=RANDOM_SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9fYbEhHgelA"
      },
      "source": [
        "### Experiment 1: LinearRegression with transformed values\n",
        "\n",
        "Our first experiment is to rerun the linear regression model with the same features as before, but this time on the new data with the transformations applied on the previous notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5msGTsiwgelB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wesTDvkUgelB"
      },
      "outputs": [],
      "source": [
        "linear_scaled_model = LinearRegression()\n",
        "\n",
        "linear_scaled_model.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this and the following experiments, we will also use cross_val_score instead of simply predicting on the test set. This will give us a better idea of how the model will perform on unseen data, given that we are making more realistic performance estimates, and the variance of the model evaluation will be lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAh5ZQ6CgelB"
      },
      "outputs": [],
      "source": [
        "scores_linear_scaled = cross_val_score(linear_scaled_model,\n",
        "                                       Xtrain,\n",
        "                                       ytrain,\n",
        "                                       cv=10,\n",
        "                                       scoring='neg_mean_squared_error',\n",
        "                                       n_jobs=-1)\n",
        "\n",
        "scores_linear_scaled = np.sqrt(-scores_linear_scaled)\n",
        "error_percent_linear = 100 * (10**scores_linear_scaled.mean() - 1)\n",
        "std_percent_linear = 100 * (10**scores_linear_scaled.std() - 1)\n",
        "print(f'Average error is {error_percent_linear:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_linear:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'linear_scaled_model.csv'\n",
        "\n",
        "# write the array scores_linear_scaled to a csv file\n",
        "\n",
        "np.savetxt(path, scores_linear_scaled, delimiter=',')\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWJxJtLFgelB"
      },
      "source": [
        "The experiment showed no large difference in the model performance, considering the standard deviation. This means that the LinearRegression model is not sensitive to the new transformations we made on the data. We will, however, still keep the new data for the next experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO88AXX_gelB"
      },
      "source": [
        "### Experiment 2: Lasso Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Lasso Regression is similar to the Linear Regression, but it adds a regularization term to the cost function, which penalizes the model for having too many features (in this case, the L1 norm of the weights). This is useful to avoid overfitting, and also to perform feature selection, since the regularization term will make the weights of the less important features go to zero.\n",
        "\n",
        "We can define this regularization term as:\n",
        "\n",
        "$$\n",
        "\\lambda \\sum_{i=1}^{n} |w_i|\n",
        "$$\n",
        "\n",
        "Where $\\lambda$ is the regularization parameter, and $w_i$ is the weight of the $i$-th feature.\n",
        "\n",
        "As $w_i$ gets closer to zero, the regularization term will also get closer to zero, and the model will be penalized less. This means that the model will try to minimize the cost function by making the weights of the less important features go to zero, and the weights of the most important features will be kept as they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxfujoYXgelC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this and the following experiments, we will use GridSearchCV to find better values for model hyperparameters. This will allow us to perform a more thorough search in the hyperparameter space, and find the best model for our data without having to manually test different values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-FnepU-gelC"
      },
      "outputs": [],
      "source": [
        "lasso = Lasso()\n",
        "\n",
        "params = {\n",
        "    'alpha': np.logspace(-4, 0, 100),\n",
        "    'max_iter': [15000, 20000, 30000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "    'selection': ['cyclic', 'random'],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(lasso, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5OrzY5PgelC"
      },
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the best parameters for Lasso Regression, from the grid search, and use them to train a new model. Then, evaluate the model using cross_val_score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJIt31ZrgelC"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYjhHl-GgelC"
      },
      "outputs": [],
      "source": [
        "lasso_best = Lasso(**best_params)\n",
        "lasso_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8wba6J7gelC"
      },
      "outputs": [],
      "source": [
        "scores_lasso = cross_val_score(lasso_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_lasso = np.sqrt(-scores_lasso)\n",
        "error_percent_lasso = 100 * (10**scores_lasso.mean() - 1)\n",
        "std_percent_lasso = 100 * (10**scores_lasso.std() - 1)\n",
        "print(f'Average error is {error_percent_lasso:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_lasso:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'lasso_score.csv'\n",
        "\n",
        "np.savetxt(path, scores_lasso, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 3: Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Ridge Regression is similar to the Linear Regression, but it adds a regularization term to the cost function, which penalizes the model for having too many features (in this case, the L2 norm of the weights). This is useful to avoid overfitting, and also to perform feature selection, since the regularization term will make the weights of the less important features go to zero.\n",
        "\n",
        "We can define this regularization term as:\n",
        "\n",
        "$$\n",
        "\\lambda \\sum_{i=1}^{n} w_i^2\n",
        "$$\n",
        "\n",
        "Where $\\lambda$ is the regularization parameter, and $w_i$ is the weight of the $i$-th feature.\n",
        "\n",
        "Differently from the Lasso Regression, the Ridge Regression will not make the weights of the less important features go to zero, but it will make them very small (since $w_i$ is squared, it will be even smaller than in the Lasso Regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ridge = Ridge()\n",
        "\n",
        "params = {\n",
        "    'alpha': np.logspace(-4, 0, 100),\n",
        "    'fit_intercept': [True, False],\n",
        "    'copy_X': [True, False],\n",
        "    'max_iter': [15000, 20000, 30000],\n",
        "    'tol': [0.0001, 0.001],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(ridge, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ridge_best = Ridge(**best_params)\n",
        "ridge_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_ridge = cross_val_score(ridge_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_ridge = np.sqrt(-scores_ridge)\n",
        "error_percent_ridge = 100 * (10**scores_ridge.mean() - 1)\n",
        "std_percent_ridge = 100 * (10**scores_ridge.std() - 1)\n",
        "print(f'Average error is {error_percent_ridge:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_ridge:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'ridge.csv'\n",
        "\n",
        "np.savetxt(path, scores_ridge, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 4:  Decision Tree Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision trees are a type of algorithm used mostly for classification, since they are easy to interpret and visualize. However, they can also be used for regression. \n",
        "\n",
        "From our exploratory analysis, we know that most variables form a linear relationship, however it is still a good itea to test this model because it is a non-parametric model, which means that it does not make any assumptions about the data distribution. This means that it can capture relationships that linear models cannot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A decision tree is a tree where each node represents a feature (or a group of features), each link represents a decision (resulting from a feature split), and each leaf represents an output (a prediction). The tree is built by splitting the data into subsets, and then splitting it again on each of the subsets, and so on, until the subsets are small enough to be represented by a leaf.\n",
        "\n",
        "As for hyperparameters, we can specify:\n",
        "- min_samples_split: the minimum number of samples required to split an internal node\n",
        "- min_samples_leaf: the minimum number of samples required to be at a leaf node\n",
        "- min_weight_fraction_leaf: the minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node\n",
        "- max_features: the number of features to consider when looking for the best split\n",
        "- min_impurity_decrease: a node will be split if this split induces a decrease of the impurity greater than or equal to this value\n",
        "- ccp_alpha: complexity parameter used for Minimal Cost-Complexity Pruning\n",
        "\n",
        "We will leave max_depth as None, so the nodes will be expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "\n",
        "params = {\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [1, 2, 3],\n",
        "    'min_weight_fraction_leaf': [0, 0.01, 0.1],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'min_impurity_decrease': [0, 0.01, 0.1],\n",
        "    'ccp_alpha': [0, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(tree, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree_best = DecisionTreeRegressor(**best_params)\n",
        "tree_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_tree = cross_val_score(tree_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_tree = np.sqrt(-scores_tree)\n",
        "error_percent_tree = 100 * (10**scores_tree.mean() - 1)\n",
        "std_percent_tree = 100 * (10**scores_tree.std() - 1)\n",
        "print(f'Average error is {error_percent_tree:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_tree:.2f}%')\n",
        "\n",
        "path = 'tree_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_tree, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLGdIqIEgelD"
      },
      "source": [
        "### Experiment 5:  Random Forest Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The random forest is very similar to the decision tree, but it uses a technique called bagging to reduce the variance of the model, by training many decision trees on different subsets of the data, and then averaging the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqhuSZ7JgelD"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBn1PDHPgelD"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor()\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [100, 600, 1200, 1800],\n",
        "    'min_samples_split': [3],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'verbose': [0, 1, 2, 3],\n",
        "    'max_depth': [None],\n",
        "    'bootstrap': [False],\n",
        "    'min_weight_fraction_leaf': [0.0],\n",
        "    'min_samples_leaf': [1],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncBy3F-igelD"
      },
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqEhl9WZgelD",
        "outputId": "625ba6f2-518f-4c9c-e6c1-45e8f75a6d78"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s88tM9SRgelD"
      },
      "outputs": [],
      "source": [
        "rf_best = RandomForestRegressor(**best_params)\n",
        "rf_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwrovDvzgelE"
      },
      "outputs": [],
      "source": [
        "scores_rf = cross_val_score(rf_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_rf = np.sqrt(-scores_rf)\n",
        "error_percent_rf = 100 * (10**scores_rf.mean() - 1)\n",
        "std_percent_rf = 100 * (10**scores_rf.std() - 1)\n",
        "print(f'Average error is {error_percent_rf:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_rf:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'rf_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_rf, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHU1A47NgelE"
      },
      "source": [
        "### Experiment 6:  Gradient Boosting Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient boosting is a technique that combines weak learners (in this case, decision trees) to create a strong learner. It is similar to the random forest, but instead of training each tree independently, it trains each tree on the residual of the previous tree.\n",
        "\n",
        "We can define the gradient boosting algorithm as:\n",
        "\n",
        "$$\n",
        "F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^{n} L(y_i, \\gamma)\n",
        "$$\n",
        "\n",
        "$$\n",
        "F_m(x) = F_{m-1}(x) + \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\n",
        "$$\n",
        "\n",
        "Where $F_0(x)$ is the first tree (obtained by minimizing $L(y_i, \\gamma)$ over the training data), $F_m(x)$ is the $m$-th tree, $L$ is the loss function, $y_i$ is the target value, $x_i$ is the $i$-th feature vector, and $h_m(x_i)$ is the prediction of the $m$-th tree.\n",
        "\n",
        "The technique uses the gradient descent algorithm to minimize the loss function, and the trees are added sequentially, so the model is built in a stage-wise fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGfMjeHbgelE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T6lq5tBgelE"
      },
      "outputs": [],
      "source": [
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "params = {\n",
        " 'alpha': [0.9], \n",
        " 'criterion': ['friedman_mse'],\n",
        " 'learning_rate': [0.1, 0.5, 0.01], \n",
        " 'max_depth': [3, 4, 5], \n",
        " 'max_features': ['sqrt'], \n",
        " 'min_samples_leaf': [2,3], \n",
        " 'n_estimators': [1600,200], \n",
        " 'subsample': [0.9, 0.8, 0.7, 0.6], \n",
        " 'verbose': [0,1,2], \n",
        " 'warm_start': [True, False]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(gbr, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQI9A7WXgelE",
        "outputId": "20c2ffb3-69d7-44a2-8145-5c0ca0415bed"
      },
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTXDdTvPgelE",
        "outputId": "a1d2b7ba-e649-4f85-ac58-96fda1456de5"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "with open('best_params_gradient.txt', 'w') as f:\n",
        "    f.write(str(best_params))\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMFs95wQgelE",
        "outputId": "5bf2a913-83c3-4c18-f6f6-c5280c713979"
      },
      "outputs": [],
      "source": [
        "gbr_best = GradientBoostingRegressor(**best_params)\n",
        "gbr_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH_u90FUgelE",
        "outputId": "f50ed70e-93f3-450e-c221-b848fbaf92a9"
      },
      "outputs": [],
      "source": [
        "scores_gbr = cross_val_score(gbr_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_gbr = np.sqrt(-scores_gbr)\n",
        "error_percent_gbr = 100 * (10**scores_gbr.mean() - 1)\n",
        "std_percent_gbr = 100 * (10**scores_gbr.std() - 1)\n",
        "print(f'Average error is {error_percent_gbr:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_gbr:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'gradient_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_gbr, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_T2JBDzgelF"
      },
      "source": [
        "### Experiment 7:  KNN Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "KNN is a different non-parametric approach to regression. It is a lazy learning algorithm, which means that it does not learn a function from the training data, but instead memorizes the training data (that is, stores everything on memory) and waits until it is given a new data point to make a prediction.\n",
        "\n",
        "During training, the algorithm simply stores the training data. During testing, the algorithm finds the $k$ nearest neighbors of the new data point, and then averages the target values of the neighbors to make a regression prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivWgnFfggelF"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wmDaSRYgelF"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsRegressor()\n",
        "\n",
        "params = {\n",
        "    'n_neighbors': [5, 6, 7, 8],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'p': [1, 2, 3],\n",
        "    'leaf_size': [30, 40, 50]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(knn, params, cv=5, verbose=1, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPB4rVMagelF"
      },
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDd4pnOegelJ"
      },
      "outputs": [],
      "source": [
        "knn_best = KNeighborsRegressor(**best_params)\n",
        "knn_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_knn = cross_val_score(knn_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_knn = np.sqrt(-scores_knn)\n",
        "error_percent_knn = 100 * (10**scores_knn.mean() - 1)\n",
        "std_percent_knn = 100 * (10**scores_knn.std() - 1)\n",
        "print(f'Average error is {error_percent_knn:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_knn:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'knn_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_knn, delimiter=',')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 8: ElasticNetCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ElasticNet is a combination of Lasso and Ridge regressions, which adds both regularization terms (L1 and L2) to the cost function. This allows the model to learn a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
        "\n",
        "We can define this new cost function as:\n",
        "\n",
        "$$\n",
        "\n",
        "\\lambda_1 \\sum_{i=1}^{n} |w_i| + \\lambda_2 \\sum_{i=1}^{n} w_i^2\n",
        "\n",
        "$$\n",
        "\n",
        "Where $\\lambda_1$ and $\\lambda_2$ are the regularization parameters.\n",
        "\n",
        "It is important to note that ElasticNetCV is not a model, but a method that can be used to find the best values for the regularization parameters $\\lambda_1$ and $\\lambda_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNetCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "elnet = ElasticNetCV()\n",
        "\n",
        "params = {\n",
        "    'l1_ratio': [0.6, 0.3, 0.2, 0.1],\n",
        "    'eps': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
        "    'n_alphas': [100, 150, 200],\n",
        "    'copy_X': [True, False],\n",
        "    'verbose': [0,1,2]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(elnet, params, cv=5, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = grid.best_params_\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "elnet_best = ElasticNetCV(**best_params)\n",
        "elnet_best.fit(Xtrain, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores_elnet = cross_val_score(elnet_best, Xtrain, ytrain, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "scores_elnet = np.sqrt(-scores_elnet)\n",
        "error_percent_elnet = 100 * (10**scores_elnet.mean() - 1)\n",
        "std_percent_elnet = 100 * (10**scores_elnet.std() - 1)\n",
        "print(f'Average error is {error_percent_elnet:.2f}%')\n",
        "print(f'Standard deviation of error is {std_percent_elnet:.2f}%')\n",
        "\n",
        "path = DATA_DIR / 'processed' / 'elnet_scores.csv'\n",
        "\n",
        "np.savetxt(path, scores_elnet, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwv4f0XOgelK"
      },
      "source": [
        "### Retraining of best model on the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkNrXjEkgelK"
      },
      "outputs": [],
      "source": [
        "best_model = linear_scaled_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-GozjutgelK"
      },
      "outputs": [],
      "source": [
        "best_model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chossing the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def compare_scores(score1, score2):\n",
        "    t_stat, p_value = ttest_ind(score1, score2, equal_var=False)\n",
        "    print(f'T-statistic: {t_stat:.2f}')\n",
        "    print(f'p-value: {p_value:.2f}')\n",
        "    if p_value < 0.05:\n",
        "        print('The difference is statistically significant.')\n",
        "    else:\n",
        "        print('Nothing can be said about the difference.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = DATA_DIR / 'processed' / 'models_log.csv'\n",
        "\n",
        "scores = pd.read_csv(path, index_col=0)\n",
        "\n",
        "scores.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores['Accuracy'] = 100 - scores['Mean Absolute Error']\n",
        "scores.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_line_score = scores.loc['Linear Regression Scaled', 'Accuracy']\n",
        "base_line_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# running the test for all models, related to the baseline\n",
        "\n",
        "for model in scores.index:\n",
        "    if model == 'Linear Regression Scaled':\n",
        "        continue\n",
        "    print(f'Comparing {model} to baseline...')\n",
        "    compare_scores(scores.loc[model, 'Accuracy'], base_line_score)\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
